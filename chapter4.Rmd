# 4. Clustering and classification
```{r include=FALSE}
library(MASS)
library(corrplot)
library(dplyr)
library(tidyr)
library(ggplot2)
````

### 4.1-4.2 Dataset: Boston data (from MASS-package)
```{r}
data("Boston")
str(Boston)
```  

Dataset `Boston` contains information about housing values in Suburbs of Boston. The dataset describes 506 towns (rows) around Boston by 14 variables including the most common value of houses (`medv`). Numerical variables contain estimates for e.g. crime, pollution (nitrogen oxygen concentrations, `nox`), average number of rooms per home (`rm`), school size (pupil-teacher ratio, `ptratio`). There's also two nonnumerical variables (binary `chas` indicating location at riverside, and ordered index `rad` describing access to the radial highways)[^2]. 

[^2]: For a full list of data attributes, see <https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html>

### 4.3 Exploration of Boston-data  
##### 4.3a Variable distributions  
Let's start looking at variable distributions using boxplots (in combination with the gather()-function, which allows us to stack the columns of the dataset Boston (506 x 14 matrix) into a 2 x 7084 matrix, where the first column contains names of variables in Boston (`key`) and the second contains values of the variables (`value`) in each town area around Boston. We also modify the `ggplot()` code from the last weeks exercises to show a boxplot instead of histograms.  
```{r}
dim(gather(Boston))
gather(Boston) %>% ggplot(aes(key,value)) + facet_wrap("key", scales = "free") + geom_boxplot() + ggtitle("Variable distributions") + theme(plot.title = element_text(hjust=0.5)) #geom_boxplot() defines that the desired type is boxplot, and we need to change the first argument into aes(x,y) to define that we want to show distribution of values in each variable (key)
summary(Boston)
```    

From boxplots, we can see that variables are shown on very different scales, the distribution of most variables is moderately to highly skewed, and the variance also varies a lot. For example, it looks like majority of town areas are located relatively close to the radial high-way (`rad`) and have a fairly low property-tax (measured by `tax`). The `summary()` command shows us the same information visualized in boxplots.  

It might be interesting to see if distributions of `tax` or `rad` (with very wide interquantile ranges) actually consist of two populations. Let's do histograms! 

```{r}
par(mfrow=c(1,2))
hist(Boston$tax)
hist(Boston$rad)
```  

Bingo! Besides the bulk of areas with lower tax value, there are some areas with extremely high taxes. A similar distribution is observable in the distance to highways (`rad`). To be thorough, let's replace the initial boxplots by violin plots, which show multimodal distributions more clearly.  
```{r warning=FALSE}
gather(Boston) %>% ggplot(aes(key,value)) + facet_wrap("key", scales = "free") + geom_violin(draw_quantiles = c(0.5)) + ggtitle("Variable distributions") + theme(plot.title = element_text(hjust=0.5)) + geom_point(position = position_jitter(0.1),alpha=0.01) #changed geom_boxplot into a geom_violin (with dra_quantiles() specifying that I want to draw a median), and added geom_points to show that I want to show points on top of the distributions (position_jitter spreads the points to make them more visible, and alpha adds transparency so that the points don't steal the show from distributions)

```    

##### 4.3b Relationships between variables
To see which variables could potentially be related, we will calculate correlations between all variables and save it into `cor_matrix` and use this to draw a correlation plot using `corrplot()`. In the following graps, the size and intensity of circles corresponding to correlation between variables. Blue circles mean positive correlations, red circles mean negative correlations.  
```{r}
cor_matrix <- cor(Boston)
corrplot(cor_matrix,method = "circle",type="upper",cl.pos = "b",tl.pos = "d",tl.cex = 0.6)
```  

From the plot, it looks like levels of industriality of the zone is positively related to levels of nitrogen oxides and negatively to the distance from employment centers, or accessibility to radial highways is strongly related to property tax-values, for example. Additionally, proportion of old buildings (`age`) seems to be lower closer to the highways (negative association with `rad`). It looks like the value of houses can be decreased by multiple causes (lot's of red circles in the last column), but it seems positively related to the mean number of rooms (`rm`, not maybe a big surprise) in apartments.  

### 4.4 Modifications of the dataset  
##### 4.4a Standardization  
Different absolute values and variances might affect the multivariate methods, so to treat all variables equally, we will center each variable around its mean and scale it so that they have equal variances. This is a quick-and-dirty way to transform unimodal data closer to a normal distribution.  
```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
```  

Now mean of each variable is zero and the range of variables (shown by distance from min to max, which is now in the range of -5 to 10 in all variables) are more similar than in the original `Boston` (where variable ranges could be as different as 0-1 vs. 0-400).  

##### 4.4b Change numerical `crim` into a categorical `crime`  
To create four `crime`-classes of equal size, we will use quantiles to define whether a town can be defined to have "low", "medium low", "medium high" or "high" crime, based on (standardized) per capita crime rate `crim`.  

```{r}
bins <- quantile(boston_scaled$crim) #vector defining the cutting values for different categories of crime
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high")) #create new variables crime into boston_scaled, with selected names of categories
boston_scaled <- dplyr::select(boston_scaled, -crim) #remove original crim from data
boston_scaled <- data.frame(boston_scaled, crime) #add new crime into data
```  

##### 4.4c Create training and testing datasets  
We are planning to do linear discriminant analysis, which is a multivariate classification methods, later. In order to test if the classification is correct, we will split the dataset randomly into training set (containing 80% of the towns) and testing set (containing the remaining 20% of the towns).  

```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8) #choose randomly 80% of rows (these will be in the training data set)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,] #testing dataset consists of all rows not selected for the training
```  

### 4.5 Linear discriminant analysis for crime rate  
In order to find out which variables are typically related to each of the crime classes, we will do linear discriminant analysis (LDA) on `crime` with other 13 variables as predictors, using the training dataset we created above. Besides fitting the LDA-model, we will show the results in a biplot.    

```{r}
lda.fit <- lda(crime~., data = train) #this creates the LDA-model

classes <- as.numeric(train$crime) #to avoid having to worry about factor levels or such, we will create a numeric copy (classes) of the categorical "crime" variable
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "black", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)} #function drawing arrows that will be added to the plot above to make it into a biplot

plot(lda.fit, dimen = 2,col=classes,pch=classes,cex=0.5,main="LDA for crime rate") # plotting the cases (towns) based on their distances in two dimensions. The color already separates different classes, so I reduced the symbol size (using the argument cex)
lda.arrows(lda.fit, myscale = 1.3) # add arrows (labeled with the variables they represent) making the distance plot into a biplot
```  

From this plot, we see that based on all non-crime variables, town areas with high crime can be quite clearly separated from areas with less crime, although there a few "medium-high crime" areas resembling "high crime" areas (green plots in the cluster of blue) and one blue "high crime" clustering with majority of green "medium-high crime". The long `rad` arrow tells that the variable most clearly separating "high crime" from other classes is the accessibility from the high-crime-rate-areas to the high-way.  
The other crime classes are less separated from each other based on non-crime variables, but their order from low to medium high (black-red-green) seems logical and suggests. The arrows are shorter and on top of each other, suggesting their predictive power for crime rate is lower, but it looks like high values on `zn` (related to zoning large areas for residential use) could be somewhat related to lower crime rate in the area.  

### 4.6 Does the LDA fitted above predict crime correctly?  
Based on the all non-crime variables in the test data set we will predict their crime classes, and cross-tabulate these predictions with the actual crime classes.  
```{r}
correct_classes <- test$crime # save the correct classes from test data
test <- dplyr::select(test, -crime) # remove the crime variable from test data

lda.pred <- predict(lda.fit, newdata = test) 
table(correct = correct_classes, predicted = lda.pred$class)
```  

If the prediction for crime level between town around Boston was perfect, everything but the top-left--bottom-right diagonal of this table would be zeros. Now, we can see that the `lda.fit` predicted nearly all towns with high crime correctly in the test data, but within the lower-than-high crime town areas, the predictive power of the model was lower. (The exact numbers depend on which rows were randomly selected from the original dataset, so they change every time the code is run). The difficulty of predicting the towns with lower-than-high crime was already visible in the LDA-plot based on training dataset (above): towns with low, medium-low and medium-high crime (black, red and green symbols, respectively) did not show up as distinct clusters in the plot.  

### 4.7 Clustering town areas around Boston  
In principle I think we could use the same dataset we loaded before (since we didn't do any direct changes to it), but just in case: let's reload and standardize the data.  
```{r}
data("Boston")
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
str(boston_scaled)
```  

For clustering, we will calculate Euclidean distances (smaller the distance, more similar the towns) between each pair of towns that can be created from 506 towns. Using standardized dataset means that variables with highest initial variability (like tax, ranging from 187-711 in the original data) do have much larger weight than variables measured on smaller scale (like nitrous oxide concentrations, ranging from 0.39-0.87).  
These distances are used in k-means, which we will first decide to do based on 3 clusters.  
```{r}
dist_eu <- dist(boston_scaled) # calculate distances and save them into a new object
km <-kmeans(boston_scaled, centers = 3) #kmeans algorithm
```

The algorithm `kmeans()` now looks for the optimal way to cluster the data into three groups (defined by argument `centers = 3`). It does this by trying out different group centroids, and for each of them, calculating the Euclidean distance of all samples to the group centroid. In the optimal solutions, groups are "located" so that the distance of all observations to groups centroids are minimal.   
The problem is that the algorithm doesn't know if some other number of clusters would allow an even better way to group data, so we will explore this by calculating the within cluster sum of squares (WCSS, kind of a multivariate variance related to each solution) for solutions with 1-10 clusters and plot them to see when the WSCC drops.   

```{r}
set.seed(1988) #unless specified otherwise, kmeans() selects a random location for each cluster it makes when calculating twcss; this could in principle affect the plot, so we will tell it to use this number instead
k_max <- 30 #max number of clusters - if the plot doesn't seem to stabilize we could set it higher
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss}) #create a vector containing total WCSS-values calculated for each 1-10 solution of kmeans
qplot(x = 1:k_max, y = twcss, geom = 'line') #plot total WCSS against number of clusters
```

This case does not seem as clear as the DataCamp example, since the curve doesn't become "bumpy" or really start to level before k=5, but the most abrupt drop of the curve ends somewhere between 2 and 3, so we will select to do clustering with k=3. (A number as high as 5 could maybe be argumented for, but to keep interpretation simpler, we will take the more parsimonous solution.)  
We will run the `kmeans()` again with this number of clusters and plot the solution to see how our data was grouped.  
```{r}
km_3 <-kmeans(boston_scaled, centers = 3) 
pairs(boston_scaled[1:5],col=km_3$cluster)
pairs(boston_scaled[6:10],col=km_3$cluster)
pairs(boston_scaled[11:14],col=km_3$cluster)
```

* Dividing the data into 3 clusters shows that high crime rate is mostly found in the cluster marked with black. Other variables typical to this cluster seem to be high values in `tax` and `rad`, for example. (So, high crime rate in town might be correlated with high-value properties maybe describing wealthiness, and either high or low accessibility to the highways, depending on what accessibility index 1 meant. 

* Red cluster seems to be related to low `tax`, high `dist`, low `nox` or `ind` and high `medv`. This could support the interpretation that these are residential areas, from where people commute to industrial/commercial centers.

* Green cluster consists of houses by the river (`chas=1`), but it is tricky to see other similarities in these towns, except potentially the low high pupil to teacher ratio (`ptratio`)  

A potentially noteworthy thing is that the red and black clusters seem to be separate fairly well when the value of houses (measured by `medv`) is plotted against `lstat`, which is explained as "lower status of the population". Without knowing how this variable is defined (or going to the potential problems of defining some people to have lower status than the others), it would look like the clustering we did relates to the higher/lower valued areas around Boston, and the visual inspection of data looks like the population with `higher/lower status` is unevenly distributed in these areas. This could represent a case of regional (economical, societal) segregation. Alternative explanation is related to zonation of different towns to either commercial/industrial (black) or residential (red) areas.   