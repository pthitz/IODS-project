# 4. Clustering and classification
```{r include=FALSE}
library(MASS)
library(corrplot)
library(dplyr)
library(tidyr)
library(ggplot2)
library(plotly)
````

### 4.1-4.2 Dataset: Boston data (from MASS-package)
```{r}
data("Boston")
str(Boston)
```  

Dataset `Boston` contains information about housing values in Suburbs of Boston. The dataset describes 506 towns (rows) around Boston by 14 variables including the most common value of houses (`medv`). Numerical variables contain estimates for e.g. crime, pollution (nitrogen oxygen concentrations, `nox`), average number of rooms per home (`rm`), school size (pupil-teacher ratio, `ptratio`). There's also two nonnumerical variables (binary `chas` indicating location at riverside, and ordered index `rad` describing access to the radial highways)[^2]. 

[^2]: For a full list of data attributes, see <https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html>

### 4.3 Exploration of Boston-data  
##### 4.3a Variable distributions  
Let's start looking at variable distributions using boxplots (in combination with the gather()-function, which allows us to stack the columns of the dataset Boston (506 x 14 matrix) into a 2 x 7084 matrix, where the first column contains names of variables in Boston (`key`) and the second contains values of the variables (`value`) in each town area around Boston. We also modify the `ggplot()` code from the last weeks exercises to show a boxplot instead of histograms.  
```{r}
dim(gather(Boston))
gather(Boston) %>% ggplot(aes(key,value)) + facet_wrap("key", scales = "free") + geom_boxplot() + ggtitle("Variable distributions") + theme(plot.title = element_text(hjust=0.5)) #geom_boxplot() defines that the desired type is boxplot, and we need to change the first argument into aes(x,y) to define that we want to show distribution of values in each variable (key)
summary(Boston)
```    

From boxplots, we can see that variables are shown on very different scales, the distribution of most variables is moderately to highly skewed, and the variance also varies a lot. For example, it looks like majority of town areas are located relatively close to the radial high-way (`rad`) and have a fairly low property-tax (measured by `tax`). The `summary()` command shows us the same information visualized in boxplots.  

It might be interesting to see if distributions of `tax` or `rad` (with very wide interquantile ranges) actually consist of two populations. Let's do histograms! 

```{r}
par(mfrow=c(1,2))
hist(Boston$tax)
hist(Boston$rad)
```  

Bingo! Besides the bulk of areas with lower tax value, there are some areas with extremely high taxes. A similar distribution is observable in the distance to highways (`rad`). To be thorough, let's replace the initial boxplots by violin plots, which show multimodal distributions more clearly.  
```{r warning=FALSE}
gather(Boston) %>% ggplot(aes(key,value)) + facet_wrap("key", scales = "free") + geom_violin(draw_quantiles = c(0.5)) + ggtitle("Variable distributions") + theme(plot.title = element_text(hjust=0.5)) + geom_point(position = position_jitter(0.1),alpha=0.01) #changed geom_boxplot into a geom_violin (with dra_quantiles() specifying that I want to draw a median), and added geom_points to show that I want to show points on top of the distributions (position_jitter spreads the points to make them more visible, and alpha adds transparency so that the points don't steal the show from distributions)

```    

##### 4.3b Relationships between variables
To see which variables could potentially be related, we will calculate correlations between all variables and save it into `cor_matrix` and use this to draw a correlation plot using `corrplot()`. In the following graps, the size and intensity of circles corresponding to correlation between variables. Blue circles mean positive correlations, red circles mean negative correlations.  
```{r}
cor_matrix <- cor(Boston)
corrplot(cor_matrix,method = "circle",type="upper",cl.pos = "b",tl.pos = "d",tl.cex = 0.6)
```  

From the plot, it looks like levels of industriality of the zone is positively related to levels of nitrogen oxides and negatively to the distance from employment centers, or accessibility to radial highways is strongly related to property tax-values, for example. Additionally, proportion of old buildings (`age`) seems to be lower closer to the highways (negative association with `rad`). It looks like the value of houses can be decreased by multiple causes (lot's of red circles in the last column), but it seems positively related to the mean number of rooms (`rm`, not maybe a big surprise) in apartments.  

### 4.4 Modifications of the dataset  
##### 4.4a Standardization  
Different absolute values and variances might affect the multivariate methods, so to treat all variables equally, we will center each variable around its mean and scale it so that they have equal variances. This is a quick-and-dirty way to transform unimodal data closer to a normal distribution.  
```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
```  

Now mean of each variable is zero and the range of variables (shown by distance from min to max, which is now in the range of -5 to 10 in all variables) are more similar than in the original `Boston` (where variable ranges could be as different as 0-1 vs. 0-400).  

##### 4.4b Change numerical `crim` into a categorical `crime`  
To create four `crime`-classes of equal size, we will use quantiles to define whether a town can be defined to have "low", "medium low", "medium high" or "high" crime, based on (standardized) per capita crime rate `crim`.  

```{r}
bins <- quantile(boston_scaled$crim) #vector defining the cutting values for different categories of crime
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high")) #create new variables crime into boston_scaled, with selected names of categories
boston_scaled <- dplyr::select(boston_scaled, -crim) #remove original crim from data
boston_scaled <- data.frame(boston_scaled, crime) #add new crime into data
```  

##### 4.4c Create training and testing datasets  
We are planning to do linear discriminant analysis, which is a multivariate classification methods, later. In order to test if the classification is correct, we will split the dataset randomly into training set (containing 80% of the towns) and testing set (containing the remaining 20% of the towns).  

```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8) #choose randomly 80% of rows (these will be in the training data set)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,] #testing dataset consists of all rows not selected for the training
```  

### 4.5 Linear discriminant analysis for crime rate  
In order to find out which variables are typically related to each of the crime classes, we will do linear discriminant analysis (LDA) on `crime` with other 13 variables as predictors, using the training dataset we created above. Besides fitting the LDA-model, we will show the results in a biplot.    

```{r}
lda.fit <- lda(crime~., data = train) #this creates the LDA-model

classes <- as.numeric(train$crime) #to avoid having to worry about factor levels or such, we will create a numeric copy (classes) of the categorical "crime" variable
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "black", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)} #function drawing arrows that will be added to the plot above to make it into a biplot

plot(lda.fit, dimen = 2,col=classes,pch=classes,cex=0.5,main="LDA for crime rate") # plotting the cases (towns) based on their distances in two dimensions. The color already separates different classes, so I reduced the symbol size (using the argument cex)
lda.arrows(lda.fit, myscale = 1.3) # add arrows (labeled with the variables they represent) making the distance plot into a biplot
```  

From this plot, we see that based on all non-crime variables, town areas with high crime can be quite clearly separated from areas with less crime, although there a few "medium-high crime" areas resembling "high crime" areas (green plots in the cluster of blue) and one blue "high crime" clustering with majority of green "medium-high crime". The long `rad` arrow tells that the variable most clearly separating "high crime" from other classes is the accessibility from the high-crime-rate-areas to the high-way.  
The other crime classes are less separated from each other based on non-crime variables, but their order from low to medium high (black-red-green) seems logical and suggests. The arrows are shorter and on top of each other, suggesting their predictive power for crime rate is lower, but it looks like high values on `zn` (related to zoning large areas for residential use) could be somewhat related to lower crime rate in the area.  

### 4.6 Does the LDA fitted above predict crime correctly?  
Based on the all non-crime variables in the test data set we will predict their crime classes, and cross-tabulate these predictions with the actual crime classes.  
```{r}
correct_classes <- test$crime # save the correct classes from test data
test <- dplyr::select(test, -crime) # remove the crime variable from test data

lda.pred <- predict(lda.fit, newdata = test) 
table(correct = correct_classes, predicted = lda.pred$class)
```  

If the prediction for crime level between town around Boston was perfect, everything but the top-left--bottom-right diagonal of this table would be zeros. Now, we can see that the `lda.fit` predicted nearly all towns with high crime correctly in the test data, but within the lower-than-high crime town areas, the predictive power of the model was lower. (The exact numbers depend on which rows were randomly selected from the original dataset, so they change every time the code is run). The difficulty of predicting the towns with lower-than-high crime was already visible in the LDA-plot based on training dataset (above): towns with low, medium-low and medium-high crime (black, red and green symbols, respectively) did not show up as distinct clusters in the plot.  

### 4.7 Clustering of town areas around Boston  
In principle I think we could use the same dataset we loaded before (since we didn't do any direct changes to it), but just in case: let's reload and standardize the data.  
```{r}
data("Boston")
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
str(boston_scaled)
```  

For clustering, we will calculate Euclidean distances (smaller the distance, more similar the towns) between each pair of towns that can be created from 506 towns. Using standardized dataset means that variables with highest initial variability (like tax, ranging from 187-711 in the original data) do have much larger weight than variables measured on smaller scale (like nitrous oxide concentrations, ranging from 0.39-0.87).  
These distances are used in k-means, which we will first decide to do based on 3 clusters.  
```{r}
dist_eu <- dist(boston_scaled) # calculate distances and save them into a new object
km <-kmeans(boston_scaled, centers = 3) #kmeans algorithm
```

The algorithm `kmeans()` now looks for the optimal way to cluster the data into three groups (defined by argument `centers = 3`). It does this by trying out different group centroids, and for each of them, calculating the Euclidean distance of all samples to the group centroid. In the optimal solutions, groups are "located" so that the distance of all observations to groups centroids are minimal.   
The problem is that the algorithm doesn't know if some other number of clusters would allow an even better way to group data, so we will explore this by calculating the within cluster sum of squares (WCSS, kind of a multivariate variance related to each solution) for solutions with 1-10 clusters and plot them to see when the WSCC drops.   

```{r}
set.seed(1988) #unless specified otherwise, kmeans() selects a random location for each cluster it makes when calculating twcss; this could in principle affect the plot, so we will tell it to use this number instead
k_max <- 30 #max number of clusters - if the plot doesn't seem to stabilize we could set it higher
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss}) #create a vector containing total WCSS-values calculated for each 1-10 solution of kmeans
qplot(x = 1:k_max, y = twcss, geom = 'line') #plot total WCSS against number of clusters
```

This case does not seem as clear as the DataCamp example, since the curve doesn't become "bumpy" or really start to level before k=5, but the most abrupt drop of the curve ends somewhere between 2 and 3, so we will select to do clustering with k=3. (A number as high as 5 could maybe be argumented for, but to keep interpretation simpler, we will take the more parsimonous solution.)  
We will run the `kmeans()` again with this number of clusters and plot the solution to see how our data was grouped.  
```{r}
km_3 <-kmeans(boston_scaled, centers = 3) 
pairs(boston_scaled[1:5],col=km_3$cluster)
pairs(boston_scaled[6:10],col=km_3$cluster)
pairs(boston_scaled[11:14],col=km_3$cluster)
```

* Dividing the data into 3 clusters shows that high crime rate is mostly found in the cluster marked with black. Other variables typical to this cluster seem to be high values in `tax` and `rad`, for example. (So, high crime rate in town might be correlated with high-value properties maybe describing wealthiness, and either high or low accessibility to the highways, depending on what accessibility index 1 meant. 

* Red cluster seems to be related to low `tax`, high `dist`, low `nox` or `ind` and high `medv`. This could support the interpretation that these are residential areas, from where people commute to industrial/commercial centers.

* Green cluster consists of houses by the river (`chas=1`), but it is tricky to see other similarities in these towns, except potentially the low high pupil to teacher ratio (`ptratio`)  

A potentially noteworthy thing is that the red and black clusters seem to be separate fairly well when the value of houses (measured by `medv`) is plotted against `lstat`, which is explained as "lower status of the population". Without knowing how this variable is defined (or going to the potential problems of defining some people to have lower status than the others), it would look like the clustering we did relates to the higher/lower valued areas around Boston, and the visual inspection of data looks like the population with `higher/lower status` is unevenly distributed in these areas. This could represent a case of regional (economical, societal) segregation. Alternative explanation is related to zonation of different towns to either commercial/industrial (black) or residential (red) areas.  

### 4.8 Bonus: which variables affect most the 5-fold clustering of Boston data?   
Let's do a 5-fold k-means clustering on standardized Boston data.  
```{r}
km_5 <-kmeans(boston_scaled, centers = 5) # clustering of Boston into 5 categories
str(km_5$cluster) # 1 x 506 matrix (or vector with 506 elements), containing cluster indexes for each town
boston_scaled$cluster <- as.factor(km_5$cluster) # add cluster indexes (coded as factor) into 'boston_scaled' dataframe (506 x 14 matrix)
```

From visual inspection above, we can get an idea of the variables that tend to have high (e.g. `rm`, rooms per dwelling, in red) or low (e.g. `crime` in green) values within each cluster. To umambiguously explore the variables which are behind this particular clustering, we can use linear discriminant analysis to determine which (linear combination of) variables affect these five clusters the most. Now that we are not planning to use the LDA-model for predictions, there is no need to create a separate training and testing sets.

```{r}
lda_5clusters <- lda(cluster~., data = boston_scaled) #LDA-model with 'cluster' as predictor and all other variables (can use str() to check that they are numerical) as predictor variables
lda_5clusters$means #prints mean variables within each cluster

classes <- as.numeric(boston_scaled$cluster) 
plot(lda_5clusters, dimen = 2,col=classes,pch=classes,main="LDA for 5 k-means clusters") 
lda.arrows(lda_5clusters, myscale = 1.5) # playing with argument 'myscale' allows us to zoom in with the arrows, and see in which direction the less-clearly affecting variables seem to increase or decrease among towns
```

This biplot shows us that when looked in two dimensions, the towns/areas around Boston can be quite unambiguously divided into two main groups. First group contains two non-overlapping clusters, 1 and 3 (black and green, respectively), and second group contains clusters 2, 4 and 5 (red, blue and turquoise). In the latter group, clusters are somewhat separated on the second axis of the two dimensional distance plot, but there are some overlapping areas (red towns among turquoise and blue town). From this figure, it looks like the 5 clusters might be too much (and the noise is starting to affect the clusters), but we need to remember that we see only two dimensions of the data.  

The arrows of the biplot tell us that accessibility to radial highways (`rad`) is the variable that separates black (6% of areas, shown by prior probabilities in the printout of the model above) and green clusters (24% of areas) from others. The second direction of variability (separating dark blue from red cluster) is composed mainly of the variable `ind` (proportion of non-retail business acres, i.e., area zoned for industry per town), and in smaller extent, by variable `nox`.  

So, based on this analysis clusters 1 (black) and 3 (green) represent areas with small index of accessibility to the radial highways. Comparing the group means for each cluster, the towns in black cluster 1 have the highest crime rate, and quite small apartments (low `rm`, rooms per dwelling) and low value of owner-occupied homes (`medv`) compared to the towns in green cluster.  

Of the towns with high index of accessibility to the radial highways, towns in the blue cluster 4 clearly rely on industry, which is partially reflected in their lower air quality (high `nox`). The turquoise (cluster 5) and red clusters (2) are mainly separated by larger proportion of very large residential areas (`zn`) and higher value of owner-occupied houses in the towns belonging to red cluster.  

### 4.9 Super-bonus: going beyond two dimensions 

```{r}
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling) 

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
str(matrix_product)
matrix_product$crime <- as.factor(train$crime) # add crime to the matrix product

#3D plot for crime classes
require(plotly)
plot_ly(matrix_product,x = ~LD1, y = ~LD2, z = ~LD3, color=~crime, colors=c('black','red','green','blue'),type='scatter3d',mode='marker')
```

This 3D plot corresponds to 2-dimensional *LDA for crime rate* we drew in section 4.5, except that it shows also the variation on the third linear component, LD3. By rotating the 3D figure and zooming in or out, we can find a position in which it remembles the original 2D plot.  

Let's try to draw a similar figure with colors defined by 5 clusters we used for the k-means in the bonus step.

```{r}
model_predictors1 <- dplyr::select(boston_scaled, -cluster)

# check the dimensions
dim(model_predictors1) # 506 x 14
dim(lda_5clusters$scaling) # 14 x 4

# matrix multiplication
matrix_product1 <- as.matrix(model_predictors1) %*% lda_5clusters$scaling
matrix_product1 <- as.data.frame(matrix_product1)
str(matrix_product1) # 506 x 4 variables
matrix_product1$cluster <- as.factor(boston_scaled$cluster) # add cluster to the matrix product

#3D plot for clusters
plot_ly(matrix_product1,x = ~LD1, y = ~LD2, z = ~LD3, color=~cluster, colors=c('black','red','green','blue','turquoise'),type='scatter3d',mode='marker')
```

This plot can be turned into a position resembling the 2D-plot we did for clustering of towns in step 4.8 (Bonus).  

If we turn the plots produced in this section until the axes point into the same direction, we can see that in the *crime rate 3D-plot* there is a big diffuse cluster of towns marked with black, red and green (low, medium-low and medium-high crime) plotted on the left, and a smaller group of blue towns (high crime) plotted on the right. In the *5-fold cluster 3D-plot*, it looks like LD1 similarly separates the towns into a big diffuse (clusters 2, 4, and 5, marked with blue, turquoise and red) and a smaller distinct (clusters 1 and 3, marked with green and black) groups, even if the colors are different and axis LD1 has been rotated.  

```{r}
lda.fit$scaling # effect of variables on crime rate based on 404 observations
lda_5clusters$scaling # effect of variables on 5-fold clustering based on 506 observations
```

Checking the coefficients of two different LDA-models reveals that LD1 in crime rate model (`lda.fit`) was mostly affected by `rad` (coefficient 3.2 is clearly higher than for any other variables in column LD1). Similarly, `rad` was the main variable affecting the axis LD1 in the 5-cluster model but the coefficient is negative (-5.9), which could explain the fact that LD1s of these two models seem to mirror each other. This supports the idea that the towns marked with high crime (dark blue in the earlier plot) are mostly the same as towns clustering in 1 and 3 (black and green in the latter plot).  

Additional variation to the plots arises from different sets of predictor variables and different observations used for the two LDA-analyses.
